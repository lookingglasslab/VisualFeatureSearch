{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6tUO/rlrKHJgmPXSb43b6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lookingglasslab/VisualFeatureSearch/blob/widen-support/notebooks/Interactive_Visual_Feature_Search_Caching_COCO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Visual Feature Search\n",
        "## CoCo Caching\n",
        "\n",
        "This notebook demonstrates how to build a feature cache from scratch for Visual\n",
        "Feature Search. We focus on creating a cache for a pretrained ResNet50 model when evaluated on the CoCo validation set; see our Basic Demo in our [Github repo](https://github.com/lookingglasslab/VisualFeatureSearch) for how this cached data can be used for efficient similarity search.\n",
        "\n",
        "Note: if you are creating your own cache, we highly recommend doing so on a dedicated VM or local machine rather than on Google Colab. The resulting feature data can be quite large (e.g. 1-10 GB depending on the model and dataset size), and it's difficult to export files of this size out of Colab."
      ],
      "metadata": {
        "id": "AVdWMxULRc7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "b_UFlIYwSqUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vgUKmdAm4FC"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "\n",
        "!unzip annotations_trainval2014.zip\n",
        "!unzip val2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: change to regular pip upon release\n",
        "!pip install -i https://test.pypi.org/simple/ --no-deps visualfeaturesearch==0.0.11\n",
        "!pip install zarr"
      ],
      "metadata": {
        "id": "qvARRESFp4jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import zarr\n",
        "import visualfeaturesearch as vfs\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  raise Exception('No GPU available')"
      ],
      "metadata": {
        "id": "ZKxDunGvsolx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Model Code\n",
        "\n",
        "The caching function is meant to be very easy to call once you have a trained model and a dataset for searching across. The next few cells focus on setting up the CoCo dataset and ResNet model."
      ],
      "metadata": {
        "id": "9We0Z5ggSuFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coco_ds = torchvision.datasets.CocoDetection(root='val2014/',\n",
        "                                             annFile='annotations/instances_val2014.json',\n",
        "                                             transform=vfs.data.net_transform,\n",
        "                                             target_transform=lambda x : 0)\n",
        "# the target_transform ensures that all target data are a fixed size (otherwise, the PyTorch dataloaders would raise an exception)\n",
        "\n",
        "coco_dl = torch.utils.data.DataLoader(coco_ds, batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F76Gr7M-oeaL",
        "outputId": "7880d3f2-22a3-4b30-bbee-744b769fa134"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=9.89s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet50(pretrained=True)\n",
        "model = model.cuda().eval()\n",
        "\n",
        "model_conv5 = vfs.util.FeatureHook(model, model.layer4[2].conv2)"
      ],
      "metadata": {
        "id": "ne8MZDAwpurn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0936cc31-86af-4778-b861-5cf6deffd10c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 147MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caching\n",
        "\n",
        "It's as simple as one function call! This function saves the computed features to a file using the Zarr python library."
      ],
      "metadata": {
        "id": "hAGErysdTDLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vfs.caching.precompute(coco_dl,\n",
        "                       model_conv5,\n",
        "                       cache_path='/content/ResNet_COCO_08172023_f16',\n",
        "                       array_name='conv5',\n",
        "                       device=device,\n",
        "                       dtype=np.float16)\n",
        "# we use float16 to make the resulting feature data have a much smaller memory\n",
        "# footprint (but with very similar search results)"
      ],
      "metadata": {
        "id": "41M8dBBXtHI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb7e083-ef8f-4a9a-c42b-f7ebdf97dcc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 256 / 40504\n",
            "Progress: 512 / 40504\n",
            "Progress: 768 / 40504\n",
            "Progress: 1024 / 40504\n",
            "Progress: 1280 / 40504\n",
            "Progress: 1536 / 40504\n",
            "Progress: 1792 / 40504\n",
            "Progress: 2048 / 40504\n",
            "Progress: 2304 / 40504\n",
            "Progress: 2560 / 40504\n",
            "Progress: 2816 / 40504\n",
            "Progress: 3072 / 40504\n",
            "Progress: 3328 / 40504\n",
            "Progress: 3584 / 40504\n",
            "Progress: 3840 / 40504\n",
            "Progress: 4096 / 40504\n",
            "Progress: 4352 / 40504\n",
            "Progress: 4608 / 40504\n",
            "Progress: 4864 / 40504\n",
            "Progress: 5120 / 40504\n",
            "Progress: 5376 / 40504\n",
            "Progress: 5632 / 40504\n",
            "Progress: 5888 / 40504\n",
            "Progress: 6144 / 40504\n",
            "Progress: 6400 / 40504\n",
            "Progress: 6656 / 40504\n",
            "Progress: 6912 / 40504\n",
            "Progress: 7168 / 40504\n",
            "Progress: 7424 / 40504\n",
            "Progress: 7680 / 40504\n",
            "Progress: 7936 / 40504\n",
            "Progress: 8192 / 40504\n",
            "Progress: 8448 / 40504\n",
            "Progress: 8704 / 40504\n",
            "Progress: 8960 / 40504\n",
            "Progress: 9216 / 40504\n",
            "Progress: 9472 / 40504\n",
            "Progress: 9728 / 40504\n",
            "Progress: 9984 / 40504\n",
            "Progress: 10240 / 40504\n",
            "Progress: 10496 / 40504\n",
            "Progress: 10752 / 40504\n",
            "Progress: 11008 / 40504\n",
            "Progress: 11264 / 40504\n",
            "Progress: 11520 / 40504\n",
            "Progress: 11776 / 40504\n",
            "Progress: 12032 / 40504\n",
            "Progress: 12288 / 40504\n",
            "Progress: 12544 / 40504\n",
            "Progress: 12800 / 40504\n",
            "Progress: 13056 / 40504\n",
            "Progress: 13312 / 40504\n",
            "Progress: 13568 / 40504\n",
            "Progress: 13824 / 40504\n",
            "Progress: 14080 / 40504\n",
            "Progress: 14336 / 40504\n",
            "Progress: 14592 / 40504\n",
            "Progress: 14848 / 40504\n",
            "Progress: 15104 / 40504\n",
            "Progress: 15360 / 40504\n",
            "Progress: 15616 / 40504\n",
            "Progress: 15872 / 40504\n",
            "Progress: 16128 / 40504\n",
            "Progress: 16384 / 40504\n",
            "Progress: 16640 / 40504\n",
            "Progress: 16896 / 40504\n",
            "Progress: 17152 / 40504\n",
            "Progress: 17408 / 40504\n",
            "Progress: 17664 / 40504\n",
            "Progress: 17920 / 40504\n",
            "Progress: 18176 / 40504\n",
            "Progress: 18432 / 40504\n",
            "Progress: 18688 / 40504\n",
            "Progress: 18944 / 40504\n",
            "Progress: 19200 / 40504\n",
            "Progress: 19456 / 40504\n",
            "Progress: 19712 / 40504\n",
            "Progress: 19968 / 40504\n",
            "Progress: 20224 / 40504\n",
            "Progress: 20480 / 40504\n",
            "Progress: 20736 / 40504\n",
            "Progress: 20992 / 40504\n",
            "Progress: 21248 / 40504\n",
            "Progress: 21504 / 40504\n",
            "Progress: 21760 / 40504\n",
            "Progress: 22016 / 40504\n",
            "Progress: 22272 / 40504\n",
            "Progress: 22528 / 40504\n",
            "Progress: 22784 / 40504\n",
            "Progress: 23040 / 40504\n",
            "Progress: 23296 / 40504\n",
            "Progress: 23552 / 40504\n",
            "Progress: 23808 / 40504\n",
            "Progress: 24064 / 40504\n",
            "Progress: 24320 / 40504\n",
            "Progress: 24576 / 40504\n",
            "Progress: 24832 / 40504\n",
            "Progress: 25088 / 40504\n",
            "Progress: 25344 / 40504\n",
            "Progress: 25600 / 40504\n",
            "Progress: 25856 / 40504\n",
            "Progress: 26112 / 40504\n",
            "Progress: 26368 / 40504\n",
            "Progress: 26624 / 40504\n",
            "Progress: 26880 / 40504\n",
            "Progress: 27136 / 40504\n",
            "Progress: 27392 / 40504\n",
            "Progress: 27648 / 40504\n",
            "Progress: 27904 / 40504\n",
            "Progress: 28160 / 40504\n",
            "Progress: 28416 / 40504\n",
            "Progress: 28672 / 40504\n",
            "Progress: 28928 / 40504\n",
            "Progress: 29184 / 40504\n",
            "Progress: 29440 / 40504\n",
            "Progress: 29696 / 40504\n",
            "Progress: 29952 / 40504\n",
            "Progress: 30208 / 40504\n",
            "Progress: 30464 / 40504\n",
            "Progress: 30720 / 40504\n",
            "Progress: 30976 / 40504\n",
            "Progress: 31232 / 40504\n",
            "Progress: 31488 / 40504\n",
            "Progress: 31744 / 40504\n",
            "Progress: 32000 / 40504\n",
            "Progress: 32256 / 40504\n",
            "Progress: 32512 / 40504\n",
            "Progress: 32768 / 40504\n",
            "Progress: 33024 / 40504\n",
            "Progress: 33280 / 40504\n",
            "Progress: 33536 / 40504\n",
            "Progress: 33792 / 40504\n",
            "Progress: 34048 / 40504\n",
            "Progress: 34304 / 40504\n",
            "Progress: 34560 / 40504\n",
            "Progress: 34816 / 40504\n",
            "Progress: 35072 / 40504\n",
            "Progress: 35328 / 40504\n",
            "Progress: 35584 / 40504\n",
            "Progress: 35840 / 40504\n",
            "Progress: 36096 / 40504\n",
            "Progress: 36352 / 40504\n",
            "Progress: 36608 / 40504\n",
            "Progress: 36864 / 40504\n",
            "Progress: 37120 / 40504\n",
            "Progress: 37376 / 40504\n",
            "Progress: 37632 / 40504\n",
            "Progress: 37888 / 40504\n",
            "Progress: 38144 / 40504\n",
            "Progress: 38400 / 40504\n",
            "Progress: 38656 / 40504\n",
            "Progress: 38912 / 40504\n",
            "Progress: 39168 / 40504\n",
            "Progress: 39424 / 40504\n",
            "Progress: 39680 / 40504\n",
            "Progress: 39936 / 40504\n",
            "Progress: 40192 / 40504\n",
            "Progress: 40448 / 40504\n",
            "Progress: 40504 / 40504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we open the cached data and print its metadata. We can see that the shape is what we'd expect (40,504 image features of size $512 \\times 7 \\times 7$), with a total size of 1.9 GB."
      ],
      "metadata": {
        "id": "QJ9Swbn0TjzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_store = zarr.DirectoryStore('ResNet_COCO_08172023')\n",
        "cache_root = zarr.group(store=cache_store, overwrite=False)\n",
        "cache_root['conv5'].info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "tDf1reNVQzB4",
        "outputId": "9a7fcadd-8866-4d31-c0e2-85a36fb79734"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name               : /conv5\n",
              "Type               : zarr.core.Array\n",
              "Data type          : float16\n",
              "Shape              : (40504, 512, 7, 7)\n",
              "Chunk shape        : (500, 512, 7, 7)\n",
              "Order              : C\n",
              "Read-only          : False\n",
              "Compressor         : Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)\n",
              "Store type         : zarr.storage.DirectoryStore\n",
              "No. bytes          : 2032328704 (1.9G)\n",
              "No. bytes stored   : 2027899893 (1.9G)\n",
              "Storage ratio      : 1.0\n",
              "Chunks initialized : 82/82"
            ],
            "text/html": [
              "<table class=\"zarr-info\"><tbody><tr><th style=\"text-align: left\">Name</th><td style=\"text-align: left\">/conv5</td></tr><tr><th style=\"text-align: left\">Type</th><td style=\"text-align: left\">zarr.core.Array</td></tr><tr><th style=\"text-align: left\">Data type</th><td style=\"text-align: left\">float16</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(40504, 512, 7, 7)</td></tr><tr><th style=\"text-align: left\">Chunk shape</th><td style=\"text-align: left\">(500, 512, 7, 7)</td></tr><tr><th style=\"text-align: left\">Order</th><td style=\"text-align: left\">C</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">False</td></tr><tr><th style=\"text-align: left\">Compressor</th><td style=\"text-align: left\">Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)</td></tr><tr><th style=\"text-align: left\">Store type</th><td style=\"text-align: left\">zarr.storage.DirectoryStore</td></tr><tr><th style=\"text-align: left\">No. bytes</th><td style=\"text-align: left\">2032328704 (1.9G)</td></tr><tr><th style=\"text-align: left\">No. bytes stored</th><td style=\"text-align: left\">2027899893 (1.9G)</td></tr><tr><th style=\"text-align: left\">Storage ratio</th><td style=\"text-align: left\">1.0</td></tr><tr><th style=\"text-align: left\">Chunks initialized</th><td style=\"text-align: left\">82/82</td></tr></tbody></table>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}